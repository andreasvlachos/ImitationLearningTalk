{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Interpretation and connections</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Why does IL work?\n",
    "\n",
    "A bit of theory (Goodman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "<ul style=\"float: left;\">\n",
    "<li>states defined via features</li>\n",
    "<li>the agent is a classifier</li>\n",
    "<li>rewards?</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<a href=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node28.html\"><img src=\"images/RL_sutton.png\" style=\"width:40%; float: right;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Inverse reinforcement learning**\n",
    "\n",
    "- we have the expert policy (inferred from the gold standard in the training data)\n",
    "- we infer the per-action reward function (rollin/out)\t\t\t\t\n",
    "\n",
    "\n",
    "Replacing the expert policy in LoLS with a random one is RL ([Chang et al., 2015](https://arxiv.org/pdf/1502.02206.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Inverse reinforcement learning\n",
    "\n",
    "- and learn a policy (classifier) to generalize to unseen data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bandit learning\n",
    "\n",
    "Rolling out each action can be expensive:\n",
    "- many possible actions\n",
    "- expensive loss functions to calculate\n",
    "\n",
    "What if we could only roll-out one? **Bandit Learning**\n",
    "\n",
    "<img src=\"images/bandits.jpg\" style=\"width:50%;\">\n",
    "\n",
    "[Sokolov et al. (2016)](http://www.cl.uni-heidelberg.de/~sokolov/pubs/sokolov16learning.pdf) used it to learn chunking/MT re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Semi/Unsupervised learning\n",
    "\n",
    "Learning with non-decomposable loss functions means\n",
    "- no need to know the correct actions\n",
    "- learn to predict them in order to minimize the loss\n",
    "\n",
    "<img src=\"images/tikz/semParse.png\" style=\"width:80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "UNSEARN ([Daumé III, 2009](http://www.umiacs.umd.edu/~hal/docs/daume09unsearn.pdf)): Predict the structured output so that you can predict the input from it (auto-encoder!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Negative data sampling\n",
    "\n",
    "<img src=\"images/tikz/posImitClassifierTraining.png\" style=\"width:55%; align:left;\">\n",
    "\n",
    "- Expert action sequence → positive example\n",
    "- All other action sequences → negative examples\n",
    "- Using all negative examples inefficient\n",
    "\n",
    "Instead generate useful negative samples around the expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A form of **Adversarial training** ([Ho and Ermon, 2016](https://arxiv.org/abs/1606.03476)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coaching\n",
    "\n",
    "<p style=\"float: left;\">If the optimal action is difficult<br>to predict, the coach teaches<br>a good one that is easier<br> (<a href=\"https://papers.nips.cc/paper/4545-imitation-learning-by-coaching.pdf\">He et al., 2012</a>)</p> <a href=\"https://commons.wikimedia.org/wiki/File:US_Navy_091206-N-2013O-023_Sam_Givens,_a_player_for_the_Harlem_Ambassadors_basketball_team,_demonstrates_proper_dribbling_techniques_to_a_boy_during_a_basketball_camp_sponsored_by_Yokosuka%27s_Morale,_Welfare_and_Recreation_Youth.jpg\"><img src=\"images/coaching.jpg\" style=\"width:35%; float: right;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Expert: $\\alpha^{\\star}= \\mathop{\\arg \\min}_{\\alpha \\in {\\cal A}} L(S_t(\\alpha,\\pi^{\\star}),\\mathbf{y})$\n",
    "\n",
    "Coach: $\\alpha^{\\star}= \\mathop{\\arg \\min}_{\\alpha \\in {\\cal A}} L(S_t(\\alpha,\\pi^{\\star}),\\mathbf{y}) - \\lambda f(\\alpha, \\mathbf{x}, S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Ranking\n",
    "\n",
    "Jana Rao Doppa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> What about Recurrent Neural Networks?</h3>\n",
    "\n",
    "<img src=\"images/rnn.png\" width=\"70%\"  style=\"background:none;\" />\n",
    "\n",
    "<p>They face similar problems:\n",
    "\t\t\t\t<ul>\n",
    "\t\t\t\t<li>trained at the word rather than sentence level</li>\n",
    "\t\t\t\t<li>assume previous predictions are correct</li>\n",
    "\t\t\t</ul>\n",
    "\t\t\t</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Imitation learning and RNNs\n",
    "\n",
    "<img src=\"images/mixer.png\" width=\"70%\"  style=\"background:none;\" />\n",
    "\n",
    "\n",
    "- DAgger mixed rollins, similar to scheduled sampling ([Bengio et al., 2015](http://arxiv.org/abs/1506.03099))\n",
    "- MIXER  (<a href=\"https://arxiv.org/abs/1511.06732\">Ranzato et al., 2016</a>): Mix REINFORCE-ment learning with imitation: we have the expert policy!\n",
    "- no rollouts, learn a  regressor to estimate action costs\n",
    "- end-to-end back propagation through the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actor-critic\n",
    "\n",
    "![](images/actorcritic.png)\n",
    "\n",
    "[Bahdanau et al. (2017)](https://arxiv.org/pdf/1607.07086.pdf):\n",
    "- actor: the RNN we are learning to use during testing\n",
    "- critic: another RNN that is trained to predict the value of the actions of the critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary so far\n",
    "\n",
    "- basic concepts\n",
    "  - loss function decomposability\n",
    "  - expert policy\n",
    "- imitation learning\n",
    "    - rollin/outs\n",
    "    - DAgger algorithm\n",
    "    - DAgger with rollouts and LoLS\n",
    "- connections and interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Changeprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### After the break\n",
    "\n",
    "- Applications:\n",
    "  - dependency parsing\n",
    "  - natural language generation\n",
    "  - semantic parsing\n",
    "- Practical advice\n",
    "    - making things faster\n",
    "    - debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<h1>Break!</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
