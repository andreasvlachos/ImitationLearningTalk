{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Applying Imitation Learning on Semantic Parsing</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Semantic parsing ###\n",
    "###### ([Goodman et al. 2016](http://aclweb.org/anthology/P16-1001)) ###### \n",
    "\n",
    "Semantic parsers map natural language to meaning representations\n",
    "- Need to abstract over syntactic phenomena, resolve anaphora, eliminate ambiguousness in word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Essentially the inverted task of natural language generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Abstract meaning representation ###\n",
    "###### ([Banarescu et al. 2013](http://www.aclweb.org/anthology/W13-2322)) ###### \n",
    "\n",
    "<div style=\"width:60%; float: left;\">\n",
    "<p style=\"float: left;\">\n",
    "<ul style=\"float: left;\">\n",
    "A meaning representation formalism that utilizes a graph to represent relationships between concepts.\n",
    "<li>Structure similar to dependency parses.</li>\n",
    "<li>But abstracts away from function words, and inflection details of words.</li>\n",
    "<li>Due to its structure, transition-based approaches are common.</li>\n",
    "</ul>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<img src=\"images/amrExample.png\" style=\"width:40%; float: right;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can Imitation Learning help with that? ### \n",
    "\n",
    "Similarly to dependency parsing, greedy encoding suffers from error propagation.\n",
    "\n",
    "And Imitation Learning addresses error propagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition system? ###\n",
    "\n",
    "We consider a dependency graph (tree) as input.\n",
    "- Dependency graphs are derived from the sentences.\n",
    "- There is a lot more training data avalaible for dependecy parsing, than exists for AMR parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transition actions transform the dependency graph into an AMR graph.\n",
    "- In intermediate stages, some nodes are labeled with words from the sentence, and others with AMR concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actions ###\n",
    "\n",
    "<img src=\"images/amrParseActions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/toBeAnimated/parse2amr_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/toBeAnimated/parse2amr_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/toBeAnimated/parse2amr_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/toBeAnimated/parse2amr_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/toBeAnimated/parse2amr_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss function? ###\n",
    "\n",
    "Smatch ([Cai and Knight, 2013](http://amr.isi.edu/smatch-13.pdf))\n",
    "- F<sub>1</sub>-Score between predicted and gold-target AMR graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computationally expensive for every rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naive Smatch doesn't calculate all possible mappings of nodes between predicted and target graphs; employs heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Expert policy? ###\n",
    "\n",
    "Best reachable state is explored via roll-outs, with naive Smatch used as a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Also, to encourage short trajectories, a length penalty is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### V-DAgger ###\n",
    "\n",
    "Variant of DAgger proposed by [Vlachos and Clark (2014)](http://www.aclweb.org/anthology/Q14-1042)\n",
    "- Employs roll-outs, with the same policy used for both roll-ins and roll-outs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imitation Learning challenges ###\n",
    "\n",
    "Incredible number of possible actions at each time-step.\n",
    "- In the order of 10<sup>3</sup> to 10<sup>4</sup>.\n",
    "- Exploring all alternative actions at each time-step can be very time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Incredible length of the action sequences.\n",
    "- In the range of 50-200 actions.\n",
    "- Especially challenging when combined with the large number of possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Targeted exploration ###\n",
    "\n",
    "There is no reason to explore alternative actions when:\n",
    "- Expert and learned policy agree on the correct action, <b>and</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- no alternative action is scored highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The algorithm limits the exploration to the expert action and learned policy actions whose scores is within a threashold $\\tau$ from the best scored one.\n",
    "- In first epoch, where there is no learned policy, we randomly explore a number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other cases of partial exploration ###\n",
    "\n",
    "SCB-LOLS and AggreVaTe both use partial exploration.\n",
    "- They select which time-step they apply it at random.\n",
    "- They select which actions they explore at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Targeted exploration focuses on the actions for which the leaned policy is least certain, or disagrees with the expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Focused costing ###\n",
    "\n",
    "Introduced by [Vlachos and Craven (2011)](http://www.aclweb.org/anthology/W/W11/W11-0307.pdf)\n",
    "- Instead of using learned policy for $\\beta$% of the rollout steps,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- use it for the first $\\beta$ steps and reverty to the expert policy for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This keeps roughly the same computational cost, while focusing the effect of the explored action to the immediate actions that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reduces noise, the mistakes the learned policy  may make on distant actions are considered irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can increase $\\beta$ with each epoch, to move away from the expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Issues of step-level stochasticity ###\n",
    "\n",
    "v-DAgger and SEARN employ step-level stochasticity during their roll-outs.\n",
    "- i.e. each step during roll-out can be performed by either the learned or expert policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In other words, the same training example may have very different roll-outs when reexamined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This results in high variance in the reward signal, and hinders effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Noise reduction ###\n",
    "\n",
    "$a$-bound by [Khardon and Wachman (2007)](http://www.jmlr.org/papers/volume8/khardon07a/khardon07a.pdf)\n",
    "- Exclude a training example from subsequent training if it has been already misclassified $a$ times during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, we could use LOLS\n",
    "- Rollouts are performed consistently with the same policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can hurt training times when moving from exclusive expert to exclusive learned policy, due to large length of action sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DAgger with a-bound ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/aboundResults.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Targeted exploration and focused costing results ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amr_ILmods.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison with previous work ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amrResults_previousWork.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison between different IL algorithms ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amrResults_otherIL.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Summary so far ### \n",
    "\n",
    "We discussed more modifications to the DAgger framework.\n",
    "- Targeted exploration consideres only actions for which the learned policy is uncertain and that disagree with the expert policy.\n",
    "- Using and $a$-bound we filter out training examples that confuse the classifier.\n",
    "- Focused costing performs the learned policy only on the actions that are immediately effected by the current explored one.\n",
    "\n",
    "We showed that imitation learning improves on the results."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
