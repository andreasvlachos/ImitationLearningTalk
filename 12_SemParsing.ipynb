{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'progress': 'true',\n",
       " 'start_slideshow_at': 'selected',\n",
       " 'theme': 'solarized',\n",
       " 'transition': 'slide'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.paths import locate_profile\n",
    "from IPython.display import Image\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'solarized',\n",
    "              'transition': 'slide',\n",
    "              'start_slideshow_at': 'selected',\n",
    "              'progress': 'true',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Noise reduction and targeted exploration</h2>\n",
    "<h3>applied on semantic parsing</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Semantic parsing ###\n",
    "\n",
    "Semantic parsers map natural language to meaning representations\n",
    "- Need to abstract over syntactic phenomena, resolve anaphora, eliminate ambiguousness in word senses\n",
    "- Essentially the inverted task of natural language generation\n",
    "\n",
    "Also known as:\n",
    "- natural language understanding\n",
    "- natural language database interfaces\n",
    "- semantic role labeling\n",
    "- question answering on databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Meaning representations ###\n",
    "\n",
    "Meaning representation examples...take some pics from Andreas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Abstract meaning representation ###\n",
    "\n",
    "A meaning representation formalism that utilizes a graph to represent relationships between concepts.\n",
    "- Structure similar to dependency parses.\n",
    "- But abstracts away from function words, and inflection details of words.\n",
    "- Due to its structure, transition-based approaches are common.\n",
    "\n",
    "<img src=\"images/amrExample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can Imitation Learning help with that? ### \n",
    "\n",
    "Similarly to dependency parsing, greedy encoding suffers from error propagation.\n",
    "\n",
    "Imitation Learning addresses error propagation, by considering the interaction among the transition being considered and transitions to be predicted later in the sentence.\n",
    "- Explores the search space, but avoids enumerating all possible outputs.\n",
    "- Also learns how to recover from errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Transition-based semantic parsing</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition from what to what? ###\n",
    "\n",
    "We consider a dependency graph (tree) as input.\n",
    "- Dependency graphs are derived from the sentences.\n",
    "- There is a lot more training data avalaible for dependecy parsing, than exists for AMR parsing.\n",
    "\n",
    "Transition actions transform the dependency graph into an AMR graph.\n",
    "- In intermediate stages, some nodes are labeled with words from the sentence, and others with AMR concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### States ###\n",
    "\n",
    "Graph\n",
    "- $x = {0,1,...N}$ are the nodes, each of them representing either one of the words in the sentence or a concept of the AMR graph\n",
    "- $a \\subseteq x \\times x \\times L$ are labeled directed arcs between the words, with labels coming from a predefined set $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### States ###\n",
    "\n",
    "- Stack $\\sigma$: initialized with all the nodes in the dependency tree, root at the bottom.\n",
    "- Buffer $\\beta$: initialized with all the children of the top node in $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actions ###\n",
    "\n",
    "<img src=\"images/amrParseActions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition-based AMR parsing in action! ###\n",
    "\n",
    "<img src=\"images/amrExample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Imitation learning for semantic parsing</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### V-DAgger ###\n",
    "\n",
    "Variant of DAgger proposed by Vlachos and Clark (2014)\n",
    "- Employs roll-outs, with the same policy used for both roll-ins and roll-outs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Challenges ###\n",
    "\n",
    "Incredible number of possible actions at each time-step.\n",
    "- In the order of 10<sup>3</sup> to 10<sup>4</sup>.\n",
    "- Exploring all alternative actions at each time-step can be very time-consuming.\n",
    "\n",
    "Incredible length of the action sequences.\n",
    "- In the range of 50-200 actions.\n",
    "- Especially challenging when combined with the large number of possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Targeted exploration ###\n",
    "\n",
    "There is no reason to explore alternative actions when:\n",
    "- Expert and learned policy agree on the correct action, <b>and</b>\n",
    "- no alternative action is scored highly.\n",
    "\n",
    "The algorithm limits the exploration to the expert action and learned policy actions whose scores is within a threashold $\\tau$ from the best scored one.\n",
    "- In first epoch, where there is no learned policy, we randomly explore a number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other cases of partial exploration ###\n",
    "\n",
    "SCB-LOLS and AggreVaTe both use partial exploration.\n",
    "- They select which time-step they apply it at random.\n",
    "- They select which actions they explore at random.\n",
    "\n",
    "Targeted exploration focuses on the actions for which the leaned policy is least certain, or disagrees with the expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Issues of step-level stochasticity ###\n",
    "\n",
    "v-DAgger and SEARN employ step-level stochasticity during their roll-outs.\n",
    "- i.e. each step during roll-out can be performed by either the learned or expert policy.\n",
    "- In other words, the same training example may have very different roll-outs when reexamined.\n",
    "- This results in high variance in the reward signal, and hinders effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Noise reduction ###\n",
    "\n",
    "$a$-bound by Khardon and Wachman (2007)\n",
    "- Exclude a training example from subsequent training if it has been already misclassified $a$ times during training.\n",
    "\n",
    "Alternatively, we could use LOLS\n",
    "- Rollouts are performed consistently with the same policy.\n",
    "- Can hurt training times when moving from exclusive expert to exclusive learned policy, due to large length of action sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Focused costing ###\n",
    "\n",
    "Introduced by Vlachos and Craven (2011)\n",
    "- Instead of using learned policy for $\\beta$% of the rollout steps, \n",
    "- use it for the first $\\beta$ steps and reverty to the expert policy for the rest.\n",
    "\n",
    "This keeps roughly the same computational cost, while focusing the effect of the explored action to the immediate actions that follow.\n",
    "- Reduces noise, the mistakes the learned policy  may make on distant actions are considered irrelevant.\n",
    "- We can increase $\\beta$ with each epoch, to move away from the expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Expert policy ###\n",
    "\n",
    "Smatch (Cai and Knight, 2013)\n",
    "- F<sub>1</sub>-Score between predicted and gold-target AMR graphs.\n",
    "- Computationally expensive for every rollout.\n",
    "\n",
    "Naive Smatch used during training\n",
    "- Skips combinatorial mapping of nodes between predicted and target graphs.\n",
    "- Also, to encourage short trajectories, a length penalty is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Semantic parser experiments</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DAgger with a-bound ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/aboundResults.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Targeted exploration and focused costing results ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amr_ILmods.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison with previous work ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amrResults_previousWork.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison with previous work ###\n",
    "\n",
    "<center>\n",
    "<img src=\"images/amrResults_otherIL.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary so far ### \n",
    "\n",
    "We discussed more modifications to the DAgger framework.\n",
    "- Targeted exploration consideres only actions for which the learned policy is uncertain and that disagree with the expert policy.\n",
    "- Using and $a$-bound we filter out training examples that confuse the classifier.\n",
    "- Focused costing performs the learned policy only on the actions that are immediately effected by the current explored one.\n",
    "\n",
    "We showed that imitation learning improves on the results."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
